Ploomber
========

.. image:: https://travis-ci.org/ploomber/ploomber.svg?branch=master
    :target: https://travis-ci.org/ploomber/ploomber.svg?branch=master

.. image:: https://readthedocs.org/projects/ploomber/badge/?version=latest
    :target: https://ploomber.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/ploomber/projects/master



Point Ploomber to your Python and SQL scripts in a ``pipeline.yaml`` file and it will figure out execution order by extracting dependencies from them.


It also keeps track of source code changes to speed up builds by skipping up-to-date tasks. This is a great way to interactively develop your projects, sync work with your team and quickly recover from crashes (just fix the bug and build again).


`Try out the live demo (no installation required) <https://mybinder.org/v2/gh/ploomber/projects/master?filepath=spec%2FREADME.md>`_.

`Click here for documentation <https://ploomber.readthedocs.io/>`_.

`Our blog <https://ploomber.io/>`_.


Works with Python 3.5 and higher.


``pipeline.yaml`` example
-------------------------

.. code-block:: yaml

    # pipeline.yaml

    # clean data from the raw table
    - source: clean.sql
      product: clean_data
      # function that returns a db client
      client: db.get_client

    # aggregate clean data
    - source: aggregate.sql
      product: agg_data
      client: db.get_client

    # dump data to a csv file
    - class: SQLDump
      source: dump_agg_data.sql
      product: output/data.csv
      client: db.get_client

    # visualize data from csv file
    - source: plot.py
      product:
        # where to save the executed notebook
        nb: output/executed-notebook-plot.ipynb
        # tasks can generate other outputs
        data: output/some_data.csv



Python script example
---------------------

.. code-block:: python

    # annotated python file (it will be converted to a notebook during execution)
    import pandas as pd

    # + tags=["parameters"]
    # this script depends on the output generated by a task named "clean"
    upstream = {'clean': None}
    product = None

    # during execution, a new cell is added here

    # +
    df = pd.read_csv(upstream['some_task'])
    # do data processing...
    df.to_csv(product['data'])


SQL script example
------------------

.. code-block:: sql

    DROP TABLE IF EXISTS {{product}};

    CREATE TABLE {{product}} AS
    -- this task depends on the output generated by a task named "clean"
    SELECT * FROM {{upstream['clean']}}
    WHERE x > 10



To run your pipeline:

.. code-block:: bash

    ploomber entry pipeline.yaml


If you build again, tasks whose source code is the same (and all
upstream dependencies) are skipped.


Start an interactive session (note the double dash):

.. code-block:: bash

    ipython -i -m ploomber.entry pipeline.yaml -- --action status


During an interactive session:


.. code-block:: python

    # visualize dependencies
    dag.plot()

    # develop your Python script interactively
    dag['task'].develop()

    # line by line debugging
    dag['task'].debug()


Install
-------

.. code-block:: shell

    pip install ploomber


To install Ploomber along with all optional dependencies:

.. code-block:: shell

    pip install "ploomber[all]"

``graphviz`` is required for plotting pipelines:

.. code-block:: shell

    # if you use conda (recommended)
    conda install graphviz
    # if you use homebrew
    brew install graphviz
    # for more options, see: https://www.graphviz.org/download/


Create a project with basic structure
-------------------------------------

.. code-block:: shell

    ploomber new


Python API
----------

There is also a Python API for advanced use cases. This API allows you build
flexible abstractions such as dynamic pipelines, where the exact number of
tasks is determined by its parameters.
